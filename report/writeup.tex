\documentclass[14pt]{article} % For LaTeX2e
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{tikz} 
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{etex}
\reserveinserts{18}
\usepackage{morefloats}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{pgfplots}

\usepackage[square, numbers]{natbib}
\usepackage[colorlinks,citecolor=red]{hyperref}
%\usepackage{algorithmicx}
%\usepackage{algorithm2e}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\usetikzlibrary{fit,positioning}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{ass}{Assumption}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{exc}{Exercise}[section]


\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}


\title{MAD Style: Multivalent Authorship Detection (MAD) Topic Models for Stylometric Analysis}

\author{David Dohan, Charles Marsh, Shubhro Saha, Max Simchowitz}
\begin{document}
\maketitle
\large
\begin{abstract}
We present the \textit{MAD Topic Model}, which uses  syntactic and stylometric $n$-gram features (e.g., part-of-speech tags, meter) to extract distinctive features about author style. MAD fits separate topic models to each of these $n$-gram vocabularies and combines the models through a multi-class logistic regression classifier. MAD breaks stylistic features into topics over vocabularies, creating a compact representation of stylistic tendency among different authors. We test the MAD Topic Model on several real world corpora using a variety of $n$-gram features, including part-of-speech, syllable stress, and sequences of word lengths.
\end{abstract}

\section{Introduction}

In the \textit{authorship detection} problem, one is first given a set of documents labeled (by author) on which to train, and then asked to identify authors of anonymized text snippets \citep{Stein}. Accurate author classification is typically used for such tasks as plagiarism identification \cite{stamatatos2009intrinsic} \cite{Stein}. However, the approaches used typically fail to produce interpretable descriptions of author styles. By approaching the authorship detection problem from a topic model perspective, we provide a multivalent sLDA algorithm that both classifies anonymized text and creates a compact representation of author style.

\section{Literature Review}

Existing methods for author classification focus on careful feature engineering, incorporating synonym use, part-of-speech tags, and sentence structure \cite{stamatatos2009survey}. Classification then amounts to feeding these features through generic classifiers, most often SVMs, Logistic Regression, and Random Forests. More sophisticated methods employ feature transformations (see the Writeprints method proposed by \citet{abbasi2008writeprints}). While these techniques are very powerful for classification, they fail to produce interpretable descriptions of each author's probabilistic tendencies.

	In this paper, we use probabilistic modeling to extract better insight into the factors that differentiate literary style. Our starting point is the Latent Dirichlet Allocation topic model \cite{Blei2003}, which describes topics as categorial distributions over words, and describes documents as proportions of topics. Topic models have become extremely popular in the field of ``Digital Humanities'', and scholars in the humanities are beginning to use LDA to understand massive document corpora \cite{blei2012topic}. Here, we adapt topic modeling to extract stylistic insight. For stylometric analysis, we regard stylistic tendencies as topics in the sense that they correspond to categorical distributions over stylistic features.

	sLDA \cite{Blei2007} extends LDA to the supervised setting, where per-document topic proportions are linked to a generalized linear model response \cite{mccullagh1984generalized}. The original sLDA focused on Poisson and Gaussian responses, and \cite{wang2009simultaneous} presents an approximate inference algorithm for a softmax response with applications to image annotation. \citet{rosen2004author} incorporate authorship into the LDA framework by assigning topic distributions on a per-author, rather than per topic basis. This method neglects the possibility that author writing style may vary over documents, and is best suited to cases when documents share many authors. Thus, this paper will follow  \citet{wang2009simultaneous} and extend the softmax response LDA to incorporate separate classes of stylometric features (e.g., meter, etymology, and part-of-speech). 
 
\section{Data}
To collect data for training and testing, we wrote scrapers for Project Gutenberg, Quora, and the Nassau Weekly. We selected these three data sources for their diversity in topic, language, and length. For example, Project Gutenberg features lengthy narrative texts while Quora features shorter comments in colloquial language. With Nassau Weekly we see a mix: modern prose in a mix of narrative and editorial styles. Because of this diversity, these corpora provide ample training and testing data for our models.

We implemented our scrapers in Python. The Project Gutenberg dataset features excerpts from fiction books by five authors. The Quora dataset features about 1600 comments from roughly 100 popular Quora users. The users were selected based on online reports for ``most followed" users on the network. Because Quora is a question-answer web site, this content is mostly informative in nature. Depending on the thoroughness of a user's answer, the length can vary from a single word to several paragraphs.

The Nassau Weekly is a student-run humor/culture newspaper. Our dataset features over 550 articles from about 200 authors. The content in this dataset is largely narrative or editorial in nature, and tend to be several paragraphs in length. Interestingly, authors for the publication tend to write in vastly different tones across articles because of the unique, cultish nature of the newspaper. The challenge for our authorship models is to detect consistent features in this dataset across articles by the same author.

\section{Feature Extraction}

We incorporated six different stylometric features, each of which was composed into $n$-grams of varying sizes before being fed into the model:
\begin{enumerate}
\item Part-of-Speech (POS) tags (e.g., `Noun' for the word ``apple''). The Penn-Treebank tag set was used, and tagging itself was performed using the Maximum Entropy approach of \citet{Ratnaparkhi}.
\item Etymological tags (e.g., `Old English' for the word ``great''). Etymological information was scraped from \textit{Webster's} Dictionary \citep{Dictionary}. As etymology is inherently root-based, words absent from the dataset were first stemmatized using the method of \citet{Porter} and lemmatized using the WordNet method of \citet{Fellbaum}. If either of the results returned were present in the dictionary, their corresponding etymological tag was returned. Else, the entry with minimum Levenshtein distance \citep{Levenshtein} was used instead.
\item Per-syllable stress (e.g., `(0, 1, 0)' for ``continue", where a 1 indicates stress). Stresses were extracted from the CMU Pronouncing Dictionary \citep{Lenzo}. As with etymology, words absent from the dictionary were looked up by minimizing Levenshtein distance with the present keys.
\item Syllables-per-word (i.e., `3' for ``continue"), again looked-up in the CMU Pronouncing Dictionary \citep{Lenzo}.
\item Syllable counts, i.e., the total number of syllables between pieces of punctuation.
\item Word counts, i.e., the total number of words between pieces of punctuation.
\end{enumerate}On top of these primitives, we also developed an algorithm to extract meter, which is outlined in Section~\ref{appendix:meter} of the Appendix. In total, this composed seven stylometric features. For each document, we extracted these features and generated the relevant $2$-, $3$-, and $4$-grams (apart from meter, for which only $8$-grams were produced, as described in the Appendix).

For illustrative purposes, Table~\ref{tab:sample_ngrams} presents several common stylistic $n$-grams and corresponding examples drawn from a Quora post by Yishan Wong, the CEO of Reddit. Notice that our feature extraction heuristics correctly identify ``Bitcoin" as a two-syllable word (despite the fact that it is not in the CMU Pronouncing Dictionary). Similarly, the etymology of ``stated" (Latin) was deduced by looking up its root, ``state"; as was the etymology of ``aims" (Old French) by looking up its root, ``aim".

\begin{table}[ht] 
\centering
\begin{tabular}{ c | c | c }
  Type & $n$-gram & Matching Text \\
  \hline
  Etymology & (AS, OE, L, OF) & ``... the key stated aims..." \\
  Syllable & (1, 1, 2, 1) & ``...fact that Bitcoin is..." \\
  Part-of-Speech & (DT, JJ, NN, IN) & ``...the libertarian culture of..." \\
  Word Counts & (7, --, 2, --) & ``It has all the features of Bitcoin--technologically speaking-- ..."
\end{tabular}
\caption{Matching stylistic $4$-grams from a Quora post.}
\label{tab:sample_ngrams}
\end{table}

\section{Methods}

To explore our data, we feed the extracted features as bags of $n$-grams to a novel LDA extension, the Multivalent Authorship Detection (MAD) Topic Model. The MAD Topic Model combines the sLDA algorithm presented in \cite{wang2009simultaneous} and \cite{Blei2007}, with the Author Topic Model presented in \cite{rosen2004author}, extending both of these models to account for multiple word types.  For each word type $t$, MAD posits its own LDA topic model. Unlike conventional LDA, in which each document shares a common Dirichlet prior, MAD gives each author its own Dirichlet prior which can be optimized with coordinate descent. This differs from the Author Topic Model, which treats each author's oeuvre as one contiguous document.

Like sLDA, MAD has a multi-class regression parameter $\eta$; classes are drawn from $\text{softmax}(\eta^T\bar{z})$, where $\overline{z}$ are the average topic assignments for each work. The complete generative process is specified in Algorithm~\ref{fig:mad}, and the graphical model is show in Figure~\ref{fig:gm}. The key innovation in this model is that it is doubly supervised: first, each author has its own topic proportions, which enforce shared topics between its documents. And second, upon conditioning on the multi-class logistic regression $\eta$, the topic assignments $z$ which contribute to correct classification are given a higher likelihood.  Thus, one would expect that the \emph{more salient} features are selected during inference. It is crucial to note that, during training, authorship is thereby treated as both a known label and a random variable. In the test stage, however, we marginalize over authors, as described in Section~\ref{appendix:model} of the Appendix.

The model is fit using variational inference \cite{wainwright2008graphical} which we address in detail in the Appendix (stochastic variational inference is supported as well \cite{hoffman2013stochastic}). After fitting the model, we extract per author distributions over topics and a total corpus distribution over topics. To classify documents, we extract topic assignments by applying LDA with the model parameters fit during training, and feed the topic assignments to the logistic regression classifier. Again, discussion is left to Section~\ref{appendix:model} of the Appendix. 

\section{Evaluation}

Our implementation of MAD extended Wang's sLDA implementation by 1000 lines of C++, so preliminary testing focused on establishing the algorithm's correctness. First, we ensured that our model likelihood increases during variational inference. Next, we simulated documents according to our generative process, and verified that we could classify such documents with 100\% accuracy, provided that each document had a fairly distinct distribution over topics. 

For each of the three corpora, we tested the classification accuracy sLDA against an 80/20 train/test split. We restricted our tests to authors above a certain threshold of documents, with this threshold varying by corpus. As a benchmark, we tested sLDA against Random Forest, Logistic Regression, and SVMs, applied to the $n$-grams representations of each document. In each test, sLDA significantly outperformed random guessing, but underperformed compared to the other benchmark methods. Full results can be seen in Figure~\ref{fig:results} on Page~\pageref{fig:results}.

\section{Exploration}

While MAD did not perform particularly well for classification, it's true usefulness is in the realm of exploration: by creating topic models over vocabularies of $n$-gram stylistic features, MAD helps uncover hidden syntactic and stylometric structure both of and across individual authors.

To visualize the generated topic models, we adapted the Termite tool \citep{Termite}, which allowed us to see the marginal distributions of the various $n$-grams (for a given word type, such as part-of-speech tag) as well as the distributions of the vocabularies across the topics.

As an example of the exploratory power of MAD, consider the topic model for the `runs of syllables' feature extracted from the Gutenberg dataset, which contains long-form documents from famous authors, such as Jane Austen's \textit{Pride and Prejudice} and Mark Twain's \textit{Huckleberry Finn}. The Termite representation of the topic model can be seen in Figure~\ref{fig:termite} on Page~\pageref{fig:termite}.

MAD assigns each author its own distribution over topics. In this case, the most heavily-used topic for Jane Austen was Topic 0; distinctive features for this topic (i.e., those that do not appear in any other topic) are of the form (1, 3, 1, 1) and (3, 1, 1, 1)--that is, runs of one-syllable words broken up by a three-syllable word. Meanwhile, the most heavily-used topic for Mark Twain was Topic 27, with distinctive features of the form (1, 1, 1, `?') and (1, 1, `?', 2)--that is, questions composed of mono-syllabic phrases.

These patterns can be found quite easily in the writings of the two authors. For example, consider the famous first sentence of \textit{Pride and Prejudice}: ``It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife." This sentence contains the (1, 3, 1, 1) and (3, 1, 1, 1) patterns with the phrase ``in pos-ses-sion of a wife". In another famous sentence, ``Pride relates more to our opinion of ourselves, vanity to what we would have others think of us.", we again see the (3, 1, 1, 1) pattern with ``van-it-y to what we".

Meanwhile, the mono-syllabic question-based structure of Twain is prevalent in the Southern drawls of his main characters. For example, in Line 153, we have: ``Who is you? Whar is you?", which contains the (1, 1, 1, `?') pattern twice in immediate succession. Similarly, on Line 256, we have: ``Do \'bout him?''. Notice that the character's accent transforms the two-syllable ``about" to the mono-syllabic ``'bout", again demonstrating the topic model's ability to capture the structure of Twain's dialogue. These results are also presented in Table~\ref{tab:distinctive_ngrams}.

Further analysis could be performed on and across the topic models for part-of-speech tags, etymology, etc. However, even with this small example, the usefulness of MAD for exploratory and explanatory purposes is immediately evident.

\begin{table}[ht] 
\centering
\begin{tabular}{ c | c | c | c }
  Author & Primary Topic & Distinctive $n$-gram & Matching Text \\
  \hline
  Mark Twain & Topic 0 & (1, 1, 1, `?') & ``Who is you?", ``Do 'bout him?" \\
  Jane Austen & Topic 27 & (3, 1, 1, 1) & ``pos-ses-sion of a wife", ``van-it-y to what we" \\
\end{tabular}
\caption{Distinctive $n$-grams for `runs of syllables'.}
\label{tab:distinctive_ngrams}
\end{table}

\section{Conclusions}
 It turns out that the MAD topic is far more useful for exploratory purposes than as a classifier. Initially, we hypothesized that reducing dimension from word counts to topics was responsible for the worse perfomance. However, Logistic Regression performs roughly as well even after applying PCA (with number of principal components = total number of topics). Given the strong performance of our model on artificially data, we should reconsider the extent to which topic models in general, and the Dirichlet distribution in particular, capture the statistical properties of natural language. In the Appendix, we suggest future improvements to our model based on the Pitman Yor Process \cite{teh2006hierarchical}. Nevertheless, we hope that the MAD topic model can still serve the digital humanities community as a tool for exploring corpora of large documents.

\newpage

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
ybar,
nodes near coords,
nodes near coords align={vertical},
bar width=9pt,
enlarge x limits=0.3,
ylabel=Classification Accuracy (\%),
x tick label style={rotate=45,anchor=east},
xtick=data,
xtick pos=left,
symbolic x coords={Quora, Nass, Gutenberg},
legend style={at={(0.5,1.15)},
anchor=north,legend columns=-1},
]
% Baseline
\addplot coordinates {(Quora, 12) (Nass, 7) (Gutenberg, 20)};
% Ours
\addplot coordinates {(Quora, 20) (Nass, 22) (Gutenberg, 62)};
% SVM
\addplot coordinates {(Quora, 32) (Nass, 29) (Gutenberg, 97)};
% RF
\addplot coordinates {(Quora, 30) (Nass, 25) (Gutenberg, 94)};
% Logistic
\addplot coordinates {(Quora, 35) (Nass, 33) (Gutenberg, 96)};
\legend{Baseline, MAD, SVM, Random Forest, Logistic}
\end{axis}
\end{tikzpicture}
\caption{MAD outperforms the baseline, but does comparatively worse than other methods.}
\label{fig:results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{termite.png}
\caption{Termite visualization for `runs of syllables' $n$-grams on the Gutenberg corpus. The $x$-axis indicates topic indices, while the $y$-axis indicate $n$-grams. Size of circle corresponds to $n$-gram importance within a topic. Austen's primary topic (0) is highlighted in blue, with Twain's (27) in orange.}
\label{fig:termite}.
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{termite2.png}
\caption{Termite visualization for etymology $4$-grams on the Gutenberg corpus. Here we deleted the top 10 most common $4$-grams because these were very common among all authors. Topics 7 and 13 account for 40\% of  Austen's writing, while 9,11,12 and 29 account for 40\% of Twain's text. Note that the two are quite similar, reflecting that the vast majority of English users rely on Old English (OE) and Anglo Saxon (AS) words. The similar etymology also reflects both Austin and Twain's rather colloquial tone, as OE and AS words are regarded as more casual. When they do use the ``fancier'' latinate words, Austen tends to use latin words consecutively, whereas Twain's more casual diction separates latin words with OE and AS. While many scholars do not distinguish between OE and AS, the Merrian Webster dictionary we scraped tends to label ``connecting words'' - against, the, but - as AS. Moreso than Twain, Austen's seeming to enjoy using 3 consecutive AS words, reflecting her longer phrases. Austen also enjoys the 3-OE-word construction ``was to be''}
\label{fig:termite}.
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
  \tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 12mm]
  \tikzstyle{connect}=[-latex, thick]
  \tikzstyle{box}=[rectangle, draw=black!100]
    \node[main, fill = white!100] (theta)  {$\theta$ };
    \node[main] (alpha) [left=of theta] { $\alpha$};
    \node[main] (z) [right=of theta] {$z$};
    \node[main, fill = black!10] (w) [right=of z] {$w$};
    \node[main] (beta) [above=of z]{$\beta$};
    \node[main] (lambda) [left=of beta]{$\lambda$};
    \node[main,  fill = black!10] (y) [below=of z,  yshift = 0mm]{$y$};
    \node[main] (eta) [left=of y, xshift = -3.8mm]{$\eta$};
    \path (alpha) edge [connect] (theta)
          (theta) edge [connect] (z)
          (z) edge [connect] (w)
          (lambda) edge [connect] (beta)
          (beta) edge [connect] (w)
          (eta) edge [connect] (y)
          (z)   edge [connect] (y);
    %\draw[->] (z.east) to  [out=-50,in=-130] (y.west) ;
    \node[rectangle, inner sep=2mm, fit= (z) (w),label=below right:$N_{dt}$, yshift = 5mm, xshift=-7mm] {};
	\node[rectangle, inner sep=2mm,draw=black!100, fit= (z) (w), yshift = -.6mm] {};
	 \node[rectangle, inner sep=2mm, fit= (beta),label= right:$K_t$, yshift = -1.4mm, xshift=-1.5mm] {};
	\node[rectangle, inner sep=2mm,draw=black!100, fit= (beta), yshift = .2mm] {};
	\node[rectangle, inner sep=8mm, fit= (theta) (z) (w) (y),label=below right:D, yshift = 16mm, xshift=-1.5mm] {};
	\node[rectangle, inner sep=13mm,minimum height = 25, draw=black!100, fit = (theta) (z) (w), yshift = -10mm, xshift=6] {};
	\node[rectangle, inner sep=8mm, fit= (alpha) (theta) (z) (w) ,label=left:A, yshift = -2.8mm, xshift=4mm] {};
	\node[rectangle, inner sep=8mm, minimum width = 60, draw=black!100, fit = (alpha) (theta) (z) (w) , yshift = -1.3mm, xshift=-2.0] {};
	\node[rectangle, inner sep=4mm, fit = (lambda) (beta) (alpha) (theta) (z) (w), label=above right:T, yshift = -8.0mm, xshift=30] {};
	\node[rectangle, inner sep=4mm, minimum width = 25, draw=black!100, fit = (lambda) (beta) (alpha) (theta) (z) (w), yshift = -.2mm, xshift=4] {};
  \end{tikzpicture}
  \caption{Graphical Model for the MAD Topic Model. $\alpha$ and $\lambda$ are Dirichlet parameters governing distributions of topics and per topic distributions over words respectively. $\theta$, $z$, $\beta$ and $w$ are multinomial distributions for per-document topics, latent topic assignments, per words toics, and observed word. $y$ is a categorical GLM response, with canonical link function and parameter $\eta$. $A$ is the number of authors, $D$ the number of documents, $T$ the number of word types, $N_{dt}$ the number of words of type $t$ in document $d$, and $K_t$ the number of topics over words of type $t$.}
  \label{fig:gm}
  \end{figure}

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{MAD Generative Process}{}
 \State $T$ word types, with $K_t$ topics. $D$ documents, labelled into $A=$ classes, with $T$ separate word counts for each of the $T$ word types. Softmax parameter $\eta_t \in \mathbb{R}^{(A-1)\times\sum_{t=1}^TK_t}$, Dirichlet priors $\{\alpha_{at}\}$ and $\{\lambda_t\}$
 	
 \For{Each Word Type $t$}
 	\State Fix vocabulary dirichlet $\lambda_t$. Draw $K_t$ topics $\beta_{tk}\sim \text{Dirichlet}(\lambda_{t})$
 \EndFor
 \For{Each Author $a$}
 	\For{Each Word Type $t$}
 	 	\State Fix author topic proportions $\alpha_{at}$
 	\EndFor
 	\For{For each document $d$ written by author $a$}
		\State Draw topic proportions $\theta_{dt}$, topics assignments $z_{dtn}$, words $w_{dtn}$ $\sim\text{LDA}(\alpha_{at},\beta_t)$.
 		\State Draw document label $\sim(\text{softmax}(\sum_{t}\overline{z}_{dt}^T\eta_t))$,where $\overline{z}_{dt}$ are average topic assignments
 	\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\caption{The complete generative process for the MAD Topic Model.}
\label{fig:mad}
\end{algorithm}

\clearpage

\bibliography{writeup}
\bibliographystyle{plainnat}

\newpage

\begin{appendix}
\small

\section{Meter}
\label{appendix:meter}

Our algorithm for meter extraction is inspired by \citet{Genzel}. While meter is traditionally a poetic quality (and is treated as such in \citep{Genzel}), we brought it to prose by focusing on the classical meter styles of Iambic, Spondee, etc., all of which are based on 2- or 3-syllable `feet'. We viewed meter as a function of stresses and specifically focused on the stress $(2\times3)=6$-grams, appending these $6$-grams with two additional bits: the first to indicate distance from the latest comma, and the second to indicate distance from the latest piece of punctuation, both taken modulo 2.

In summary, we evaluate meter by producing $8$-grams: the first $6$ entries in the $8$-gram are the relevant per-syllable stresses, and the last two entries indicate even or odd positioning relative to the nearest comma or other piece of punctuation.

\section{Model Discussion}
\label{appendix:model}

We adopt the following notation: let $d$ be a document, $\mathcal{D}$ be the set of documents, and $D$ the number of documents. In general, lower case letters will denote instances, calligraphic letters sets, and upper case letters cardinalities: $t$ will correspond to word types, $n$ to words, and $k$ to topics, $a$author. We will use subscripts in the natural way: $\mathcal{D}_a$ is the set of documents writter by $a$, $K_t$ are the number of topics for type $t$.

\subsection{Parameter Fitting}
\label{appendix:parameter_fitting}

It is well known that exact inference for LDA requires computing a prohibitive integral \cite{Blei2003}. Instead, the posterior distribution $p(a,w,z,\theta|\alpha,\theta,\lambda,\eta)$ is approximated with a variational family: $q(\theta_{t,d}|\gamma_{t,d})\prod_{(t,n)\in d}q(z_{dt,n}|\phi_{dt,n})$, indexed by parameters $\gamma$ and $\phi$. Here $\theta|\gamma \sim\text{Dirichlet}(\gamma)$ and $z|\phi \sim\text{Multi}(\phi)$, so that complete conditionals of $\theta$ and $z$ under $p$ are in the same family as their variational counterparts. Up to a constant independent of the variation parameters $\phi$ and $\gamma$, the KL divergence between $p$ and $q$ gives a lower bound on the posterior log likelihood. This is known as the ELBO, and (though non-convex), can be optimized with coordinate-wise gradient ascent. The parameters of the model--notably the per-author topics --can be fit using maximum likelihood methods. The updates follow \cite{wang2009simultaneous} very closely, and in the interest of brevity, are omitted.

In each step, Wang's code reinitializes $\phi$ and $\gamma$ at each iteration \cite{wang2009simultaneous}, and then optimizes a fixed point method to convergence before updating global parameters $\beta$ and $\eta$. We also implement a method that keeps $\gamma$ from the past iteration, and then then uses fixed point methods to optimizes $\phi$. This seems to have slightly faster convergence, though it suffers a bit on classification accuracy. We also implement $L_1$ regularization with L-BFGS \cite{liu1989limited} and OWL-QN\footnote{BFGS is not guaranteed to work with $L_1$ because the objective is not smooth, though it is still found to work well with $L_1$ penalties in practice. OWL-QN was implemented but not thoroughly tested and has some external package dependencies} \cite{andrew2007scalable}.

Our code also implements a number of extensions to the algorithm in \cite{wang2009simultaneous}. First, we allow for Maximum Likelihood Estimation of the per-author and global Dirichlet parameters (optimizing the variational lower bound). Both L-BFGS and Fixed Point \cite{minka2000estimating} are supported. We also use a method which gives us the expected topic assignments for authors, if all the authors documents are treated as one combined text: that is, for each word type $t$, $Pr(\text{topic}=k) = \epsilon + \sum_{d\in \mathcal{D}_a}\phi_{nd,t}$, where $\epsilon$ is a smoothing parameter.

Finally, our code extends the original sLDA implementation by including support for stochastic variational inference (SVI) \cite{hoffman2013stochastic}. On each run, a minibatch of documents are sampled, the local parameters are computed, and then the global parameters are updated using a noisy estimation of the gradient via lines 10 and 11, Figure 6, in  \cite{hoffman2013stochastic}. Using the same minibatch, we obtain a noisy estimate of the gradient of the variational objective with respect to $\eta$, and optimize. Every few iterations, we run a non-stochastic step to speed up convergence. This method converges very slowly: we hypothesize this is because the optimization of $\eta$ relies on an approximation, second order expansion (see \cite{wang2009simultaneous}), and hence sampling may not produce unbiased estimates. We only tested the algorithm with uniform sampling, but the code is written to accomodate non-uniform sampling as well (for example, one might want a minibatch of documents for each author).


\subsection{Classification}
\label{appendix:classification}

Our model implements two classification algorithms: the first methods assigns each with LDA (using a global (not per-author) prior over topics). For each document $d$, we then feed the average topic assignments $\bar{z}_{t}=\sum_{n\in D}\phi_{dn,t}$ into our softmax classifier and rank the potential authors for $d$ using the softmax likelihood. We then output perplexity, accuracy, and recall at $2$ and $3$. The second classification method supported computes the total document likelihood under each author (using the per-author topic prior), and then chooses the author which maximizes this likelihood. In practice, the first method yields better performance, and also runs more quickly.
 
\subsection{Smoothing}
\label{appendix:smoothing}
With large vocabularies, we may encounter terms in the test set that were not present in the training set. To this end, we implemented a vocabulary smoothing factor (see Section 5.4 in \cite{Blei2003}) that is equivalent to placing a Dirichlet prior on vocabulary distributions. We also fixed a segmentation fault in Chong Wang's original code that occurred when new words in the test set were encountered.

\section{The Pitman Yor Process}
Here we pick up the discuss of Pitman Yor (PY) language models alluded to in the conclusion. The PY model is a generalizetion of the Hierarchial Dirichlet Process (HDP), which  \cite{blei2003hierarchical} uses the uses to build a nonparametric topic models which can determine the optimal number of topics from the given data, and \cite{perotte2011hierarchically} adapts sLDA to incorporate Gaussian response. Recent work out of Oxford replaces the HDP with Hierarchical Pitman-Yor (HPY) topic models, which roughly obey the power law frequencies observed from English language text \cite{teh2006hierarchical}. That is, the Dirichlet distribution spreads out topic and word proportions far more equitably than one would see in natural language. Due to this insight, the HPY process underlies various various Bayesian natural language models, including Adaptor Grammars \cite{johnson2007adaptor} and the Sequence Memoizer\cite{wood2009stochastic}  Unforunately, the time constraints of this project necessitated that we stick with parametric topic models. \footnote{In particular, fixing the number of topics yields models speeds up inference drastically. Nonparametrics are generally fit with Gibbs Sampling, whereas parametric models adopt the substantially faster method of variational inference \cite{wainwright2008graphical}}. Nevertheless, one might hope that supervised Pitman-Yor topic model would provide a more faithful representation of natural language structure than LDA is able to capture. 


\end{appendix}

\end{document}
